{
 "metadata": {
  "name": "",
  "signature": "sha256:5d7799f9b874f4db00d800dfd9ff94b17ba0c6b19aca3fb14b529f329453f08c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Looking here and there -- Searchlights"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "In *chap_tutorial_classifiers* we have seen how we can implement a\n",
      "classification analysis, but we still have no clue about where in the brain (or\n",
      "our chosen ROIs) our signal of interest is located.  And that is despite the\n",
      "fact that we have analyzed the data repeatedly, with different classifiers and\n",
      "investigated error rates and confusion matrices. So what can we do?\n",
      "\n",
      "Ideally, we would like to have some way of estimating a score for each feature\n",
      "that indicates how important that particular feature (most of the time a\n",
      "voxel) is in the context of a certain classification task. There are various\n",
      "possibilities to get a vector of such per-feature scores in PyMVPA. We could\n",
      "simply compute an [ANOVA](http://en.wikipedia.org/wiki/Analysis_of_variance) F-score per each feature, yielding scores that would\n",
      "tell us which features vary significantly between any of the categories in our\n",
      "dataset.\n",
      "\n",
      "Before we can take a look at the implementation details, let's first recreate\n",
      "our preprocessed demo dataset. The code is very similar to that from\n",
      "*chap_tutorial_classifiers* and should raise no questions. We get a\n",
      "dataset with one sample per category per run."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from mvpa2.tutorial_suite import *\n",
      "ds = get_haxby2001_data(roi='vt')\n",
      "ds.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "(16, 577)"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Measures"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "Now that we have the dataset, computing the desired ANOVA F-scores is\n",
      "relatively painless:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "aov = OneWayAnova()\n",
      "f = aov(ds)\n",
      "print f"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<Dataset: 1x577@float64, <fa: fprob>>\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/lib/python2.7/dist-packages/numpy/lib/utils.py:134: DeprecationWarning: `fprob` is deprecated!\n",
        "fprob is deprecated in scipy 0.14, use stats.f.sf or special.fdtrc instead\n",
        "\n",
        "  warnings.warn(depdoc, DeprecationWarning)\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "If the code snippet above is of no surprise then you probably got the basic\n",
      "idea. We created an object instance `aov` being a\n",
      "[OneWayAnova](http://pymvpa.org/generated/mvpa2.measures.anova.OneWayAnova.html#mvpa2-measures-anova-onewayanova). This instance is subsequently\n",
      "*called* with a dataset and yields the F-scores wrapped into a\n",
      "[Dataset](http://pymvpa.org/generated/mvpa2.datasets.base.Dataset.html#mvpa2-datasets-base-dataset). Where have we seen this before?  Right!\n",
      "This one differs little from a call to\n",
      "[CrossValidation](http://pymvpa.org/generated/mvpa2.measures.base.CrossValidation.html#mvpa2-measures-base-crossvalidation).  Both are objects that get\n",
      "instantiated (potentially with some custom arguments) and yield the results in\n",
      "a dataset when called with an input dataset. This is called a [processing](http://pymvpa.org/glossary.html#term-processing) and is a common concept in PyMVPA.\n",
      "\n",
      "However, there is a difference between the two processing objects.\n",
      "[CrossValidation](http://pymvpa.org/generated/mvpa2.measures.base.CrossValidation.html#mvpa2-measures-base-crossvalidation) returns a dataset with a single\n",
      "feature -- the accuracy or error rate, while\n",
      "[OneWayAnova](http://pymvpa.org/generated/mvpa2.measures.anova.OneWayAnova.html#mvpa2-measures-anova-onewayanova) returns a vector with one value per\n",
      "feature. The latter is called a\n",
      "[FeaturewiseMeasure](http://pymvpa.org/generated/mvpa2.measures.base.FeaturewiseMeasure.html#mvpa2-measures-base-featurewisemeasure). But other than the number of\n",
      "features in the returned dataset there is not much of a difference. All\n",
      "measures in PyMVPA, for example, support an optional post-processing step.\n",
      "During instantiation of a measure an arbitrary mapper can be specified to be\n",
      "called internally to forward-map the results before they are returned. If, for\n",
      "some reason, the F-scores need to be scaled into the interval [0,1], an\n",
      "[FxMapper](http://pymvpa.org/generated/mvpa2.mappers.fx.FxMapper.html#mvpa2-mappers-fx-fxmapper) can be used to achieve that:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "aov = OneWayAnova(\n",
      "        postproc=FxMapper('features',\n",
      "                          lambda x: x / x.max(),\n",
      "                          attrfx=None))\n",
      "f = aov(ds)\n",
      "print f.samples.max()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.0\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- - -\n",
      "**Exercise**\n",
      "\n",
      "\n",
      "\n",
      "Map the F-scores back into a brain volume and look at their distribution\n",
      "in the ventral temporal ROI."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# you can use this cell for this exercise\n",
      "map2nifti(ds, f).to_filename('/tmp/f.nii.gz')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- - -\n",
      "\n",
      "\n",
      "Now that we know how to compute feature-wise F-scores we can start worrying\n",
      "about them. Our original goal was to decipher information that is encoded\n",
      "in the multivariate pattern of brain activation. But now we are using an\n",
      "ANOVA, a **univariate** measure, to localize important voxels? There must\n",
      "be something else -- and there is!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Searching, searching, searching, ..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "*Kriegeskorte et al. (2006)* suggested an algorithm that takes\n",
      "a small, sphere-shaped neighborhood of brain voxels and computes a\n",
      "multivariate measure to quantify the amount of information encoded in its\n",
      "pattern (e.g.  [mutual information](http://en.wikipedia.org/wiki/Mutual_information)). Later on this [searchlight](http://pymvpa.org/glossary.html#term-searchlight)\n",
      "approach has been extended to run a full classifier cross-validation in\n",
      "every possible sphere in the brain. Since that, numerous studies have\n",
      "employed this approach to localize relevant information in a locally\n",
      "constraint fashion.\n",
      "\n",
      "We know almost all pieces to implement a searchlight analysis in\n",
      "PyMVPA. We can load and preprocess datasets, we can set up a\n",
      "cross-validation procedure."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = kNN(k=1, dfx=one_minus_correlation, voting='majority')\n",
      "cv = CrossValidation(clf, HalfPartitioner())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "The only thing left to do is that we have to split the dataset into all\n",
      "possible sphere neighborhoods that intersect with the brain. To achieve this,\n",
      "we can use [sphere_searchlight()](http://pymvpa.org/generated/mvpa2.measures.searchlight.sphere_searchlight.html#mvpa2-measures-searchlight-sphere-searchlight):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sl = sphere_searchlight(cv, radius=3, postproc=mean_sample(), nproc=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "This single line configures a searchlight analysis that runs a full\n",
      "cross-validation in every possible sphere in the dataset. Each sphere has a\n",
      "radius of three voxels. The algorithm uses the coordinates (by default\n",
      "`voxel_indices`) stored in a feature attribute of the input dataset to\n",
      "determine local neighborhoods. From the `postproc` argument you might\n",
      "have guessed that this object is also a measure -- and your are right. This\n",
      "measure returns whatever value is computed by the basic measure (here this\n",
      "is a cross-validation) and assigns it to the feature representing the\n",
      "center of the sphere in the output dataset. For this initial example we are\n",
      "not interested in the full cross-validation output (error per each fold),\n",
      "but only in the mean error, hence we are using an appropriate mapper for\n",
      "post-processing. As with any other [processing object](http://pymvpa.org/glossary.html#term-processing-object) we have to\n",
      "call it with a dataset to run the actual analysis:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res = sl(ds)\n",
      "print res"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<Dataset: 1x577@float64, <sa: cvfolds>, <fa: center_ids>, <a: mapper>>\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "That was it. However, this was just a toy example with only our ventral\n",
      "temporal ROI. Let's now run it on a much larger volume, so we can actually\n",
      "localize something (even loading and preprocessing will take a few seconds).\n",
      "We will reuse the same searchlight setup and run it on this data as well.\n",
      "Due to the size of the data it might take a few minutes to compute the\n",
      "results, depending on the number of CPUs in the system."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ds = get_haxby2001_data_alternative(roi=0)\n",
      "print ds.nfeatures"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "34888\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res = sl(ds)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "Now let's see what we got. Since a vector with 35k elements is a little\n",
      "hard to comprehend we have to resort to some statistics."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sphere_errors = res.samples[0]\n",
      "res_mean = np.mean(res)\n",
      "res_std = np.std(res)\n",
      "chance_level = 1.0 - (1.0 / len(ds.uniquetargets))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "As you'll see, the mean empirical error is just barely below the chance level.\n",
      "However, we would not expect a signal for perfect classification\n",
      "performance in all spheres anyway. Let's see for how many spheres the error\n",
      "is more the two standard deviations lower than chance."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "frac_lower = np.round(np.mean(sphere_errors < chance_level - 2 * res_std), 3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "frac_lower"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "0.083000000000000004"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "So in almost 10% of all spheres the error is substantially lower than what\n",
      "we would expect for random guessing of the classifier -- that is more than\n",
      "3000 spheres!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- - -\n",
      "**Exercise**\n",
      "\n",
      "\n",
      "\n",
      "Look at the distribution of the errors\n",
      "(hint: `hist(sphere_errors, bins=np.linspace(0, 1, 18))`.\n",
      "In how many spheres do you think the classifier actually picked up\n",
      "real signal? What would be a good value to threshold the errors to\n",
      "distinguish false from true positives? Think of it in the context of\n",
      "statistical testing of fMRI data results. What problems are we facing\n",
      "here?\n",
      "\n",
      "\n",
      "Once you are done thinking about that -- and only \n",
      "*after* you're done,\n",
      "project the sphere error map back into the fMRI volume and look at it as\n",
      "a brain overlay in your favorite viewer (hint: you might want to store\n",
      "accuracies instead of errors, if your viewer cannot visualize the lower\n",
      "tail of the distribution:\n",
      "`map2nifti(ds, 1.0 - sphere_errors).to_filename('sl.nii.gz')`).\n",
      "Did looking at the image change your mind?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# you can use this cell for this exercise\n",
      "hist(sphere_errors, bins=np.linspace(0, 1, 18))\n",
      "map2nifti(ds, 1.0 - sphere_errors).to_filename('sl.nii.gz')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD7CAYAAABjVUMJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAECxJREFUeJzt3X+s3Xddx/Hny3XTTQd1gZT90k3opCVKxpJ1IeCOirMu\nsC3G7AdxTKhEnDgkQe2IZuUfHRijI2aLorCOyEyROEeoZWXZEU1glbCNslLWokVabNHh+CH80bq3\nf5xv10O9vfe0Pfecc/t5PpKTfM/nfL7f8/7cc8/3db6fc77npKqQJLXn+6ZdgCRpOgwASWqUASBJ\njTIAJKlRBoAkNcoAkKRGLZt2AUdL4udSJek4VVWOd52ZPAKoqpm63HHHHVOvwbE63mmNtXtWzshl\n9P1DS4/tiZrJAJAkLT4DQJIaZQCMoNfrTbuEiWlprNDWeFsaK7Q33hORk5k/WgxJatZqklqWhMPz\n79OXk5rzPlUloU6VN4ElSYvPAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQ\npEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRCwZAktuTPJlke5IPJfn+JOck2ZrkqSQPJVl+\nVP9dSXYmuWqo/bJuG7uS3LVYA5IkjWbeAEhyEfBm4BVV9RPAacCNwHpga1VdAjzcXSfJauAGYDWw\nFrg7g58TArgHWFdVK4GVSdaOfTSSpJEtdATwTeAgcFaSZcBZwFeBa4CNXZ+NwHXd8rXA/VV1sKr2\nALuBNUnOBc6uqm1dv/uG1pEkTcG8AVBVXwf+GPh3Bjv+Z6pqK7Ciqg503Q4AK7rl84C9Q5vYC5w/\nR/u+rl2SNCXL5rsxyYuB3wIuAr4BfDjJLw/3qapKMtZfad6wYcNzy71ej16vN87NS9KS1u/36ff7\nJ72dVB17353kBuDnqupXu+s3A1cAPwP8dFXt76Z3HqmqlyZZD1BVd3b9twB3AF/u+qzq2m8Crqyq\nt8xxnzVfTZIma/A23qw8J4P7h/8vCVWVhXt+r4XeA9gJXJHkzO7N3NcAO4CPArd0fW4BHuiWHwRu\nTHJGkouBlcC2qtoPfDPJmm47Nw+tI0magnmngKrqiST3AZ8BngU+C/wFcDawKck6YA9wfdd/R5JN\nDELiEHDr0Mv5W4F7gTOBzVW1ZeyjkSSNbN4poGlwCkiaLU4Bzb7FmgKSJJ2iDABJapQBIEmNMgAk\nqVEGgCQ1ygCQpEYZAJLUKANAkho175nAkqbjyM9oSIvHAJBm1qyc8WoYnaqcApKkRhkAktQoA0CS\nGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1yhPBJC0ps3SW9FL/eUoDQNISMys73dkJohPlFJAkNcoA\nkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJ\napQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkho1UgAkWZ7kb5N8IcmOJGuSnJNka5KnkjyUZPlQ\n/9uT7EqyM8lVQ+2XJdne3XbXYgxIkjSaUY8A7gI2V9Uq4CeBncB6YGtVXQI83F0nyWrgBmA1sBa4\nO0m67dwDrKuqlcDKJGvHNhJJ0nFZMACSPB94dVW9H6CqDlXVN4BrgI1dt43Add3ytcD9VXWwqvYA\nu4E1Sc4Fzq6qbV2/+4bWkSRN2ChHABcD/5nkA0k+m+R9SX4QWFFVB7o+B4AV3fJ5wN6h9fcC58/R\nvq9rlyRNwbIR+7wCeGtV/UuSP6Wb7jmsqipJjauoDRs2PLfc6/Xo9Xrj2rQkLXn9fp9+v3/S20nV\n/PvtJC8CPlVVF3fXXwXcDvwY8NNVtb+b3nmkql6aZD1AVd3Z9d8C3AF8ueuzqmu/Cbiyqt5y1P3V\nQjVJp7rB22az8jywlrmFWdlXJaGqsnDP77XgFFBV7Qe+kuSSruk1wJPAR4FburZbgAe65QeBG5Oc\nkeRiYCWwrdvON7tPEAW4eWgdSdKEjTIFBPCbwF8nOQP4EvBG4DRgU5J1wB7geoCq2pFkE7ADOATc\nOvSS/lbgXuBMBp8q2jKmcUiSjtOCU0CT5hSQ5BTQsc1WLbOyr1q0KSBJ0qnJAJCkRhkAktQoA0CS\nGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlR\nBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktSoZdMuQJoV\nSaZdgjRRBoD0PWraBXQMIy0+p4AkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIA\nJKlRBoAkNcoAkKRGjRQASU5L8liSj3bXz0myNclTSR5Ksnyo7+1JdiXZmeSqofbLkmzvbrtr/EOR\nJB2PUY8A3gbs4Mg3Za0HtlbVJcDD3XWSrAZuAFYDa4G7c+QrFu8B1lXVSmBlkrXjGYIk6UQsGABJ\nLgCuBv6SI19ReA2wsVveCFzXLV8L3F9VB6tqD7AbWJPkXODsqtrW9btvaB1J0hSMcgTwJ8BvA88O\nta2oqgPd8gFgRbd8HrB3qN9e4Pw52vd17ZKkKZk3AJK8FvhaVT3GMb6gvKqK2fkSdUnSiBb6QZhX\nAtckuRr4AeB5ST4IHEjyoqra303vfK3rvw+4cGj9Cxi88t/XLQ+37zvWnW7YsOG55V6vR6/XG2kw\nktSCfr9Pv98/6e1k8AJ+hI7JlcA7qup1Sd4DPF1V706yHlheVeu7N4E/BFzOYIrnE8BLqqqSPArc\nBmwDPga8t6q2zHE/NWpN0jgNPq8wK/971jK32aplVvZVSaiq4/4ZueP9ScjDo70T2JRkHbAHuB6g\nqnYk2cTgE0OHgFuH9ua3AvcCZwKb59r5S5ImZ+QjgEnxCEDT4hHAsVjL3Jb+EYBnAktSowwASWqU\nASBJjTIAJKlRx/spIElS58hXnS1NBoAknbDZ+BTQMb6oYUFOAUlSowwASWqUASBJjTIAJKlRBoAk\nNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKj\nDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoA\nkKRGGQCS1CgDQJIaZQBIUqMMAElq1IIBkOTCJI8keTLJ55Pc1rWfk2RrkqeSPJRk+dA6tyfZlWRn\nkquG2i9Lsr277a7FGZIkaRSjHAEcBN5eVS8DrgB+I8kqYD2wtaouAR7urpNkNXADsBpYC9ydJN22\n7gHWVdVKYGWStWMdjSRpZAsGQFXtr6rHu+VvA18AzgeuATZ23TYC13XL1wL3V9XBqtoD7AbWJDkX\nOLuqtnX97htaR5I0YcuOp3OSi4BLgUeBFVV1oLvpALCiWz4P+PTQansZBMbBbvmwfV27Gnbk4FDS\npI0cAEl+CPgI8Laq+tbwE7eqKkktQn1qwqz86xhGastIAZDkdAY7/w9W1QNd84EkL6qq/d30zte6\n9n3AhUOrX8Dglf++bnm4fd9c97dhw4bnlnu9Hr1eb5QyJakR/e5yclI1/6uv7g3cjcDTVfX2ofb3\ndG3vTrIeWF5V67s3gT8EXM5giucTwEu6o4RHgduAbcDHgPdW1Zaj7q8WqkmnjsG/16w83tYyN2uZ\n22zVUlXHfQg7SgC8Cvgk8DmOjPZ2BjvxTcCPAHuA66vqmW6ddwJvAg4xmDL6eNd+GXAvcCawuapu\nm+P+DICGGADHYi1zs5a5LVIATJoB0BYD4FisZW7WMrcTCwDPBJakRhkAktQoA0CSGmUASFKjDABJ\napQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRG\nGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjlk27AE1e\nkmmXIGkGGADNqmkX0DGMpGlxCkiSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANA\nkhplAEhSoyYeAEnWJtmZZFeS3530/UuSBiYaAElOA/4MWAusBm5KsmqSNZyIfr9/0ttIMjOXBUZ7\n0mNdWvrTLmCC+tMuYML60y5g5k36COByYHdV7amqg8DfANdOuIbjNo4AGKgZucynP4ZxLiX9aRcw\nQf1pFzBh/WkXMPMmHQDnA18Zur63a5MkTdikvw56ot9BPM7vvX/Xu941tm1J0ixI1eT2yUmuADZU\n1dru+u3As1X17qE+s/JF9ZK0ZFTVcb/inXQALAO+CPws8FVgG3BTVX1hYkVIkoAJTwFV1aEkbwU+\nDpwG/JU7f0majokeAUiSZodnAncWOkEtSS/JN5I81l1+bxp1jssoJ+R1Y34syeeT9Cdc4liN8Pi+\nY+ix3Z7kUJLl06j1ZI0w1hck2ZLk8e6x/ZUplDkWI4z1h5P8XZInkjya5GXTqHMckrw/yYEk2+fp\n897ub/FEkksX3GhVNX9hMB21G7gIOB14HFh1VJ8e8OC0a53geJcDTwIXdNdfMO26F3O8R/V/LfCJ\nade9iI/tBuAPDz+uwNPAsmnXvkhj/SPg97vlH1+qj2tX/6uBS4Htx7j9amBzt7wG+PRC2/QIYGDU\nE9TG97nS6RplvK8HPlJVewGq6r8mXOM4He8JiK8H7p9IZeM3ylj/A3het/w84OmqOjTBGsdllLGu\nAh4BqKovAhcleeFkyxyPqvon4L/n6XINsLHr+yiwPMmK+bZpAAyMcoJaAa/sDq02J1k9serGb5Tx\nrgTOSfJIks8kuXli1Y3fyCcgJjkL+HngIxOoazGMMtb3AS9L8lXgCeBtE6pt3EYZ6xPALwIkuRz4\nUeCCiVQ3eXP9PeYd66RPBJtVo7wT/lngwqr6TpJfAB4ALlncshbNKOM9HXgFg4/sngV8Ksmnq2rX\nola2OI7nkw6vA/65qp5ZrGIW2ShjfSfweFX1krwY2Jrk5VX1rUWubdxGGeudwF1JHgO2A48B/7uo\nVU3X0bMU8/6NPAIY2AdcOHT9Qgbp+Zyq+lZVfadb/gfg9CTnTK7EsVpwvAxeSTxUVd+tqqeBTwIv\nn1B94zbKeA+7kaU7/QOjjfWVwIcBqupLwL8xmB9fakZ93r6pqi6tqjcALwT+dYI1TtLRf48LurZj\nMgAGPgOsTHJRkjOAG4AHhzskWZHuuyW6Q8lU1dcnX+pYLDhe4O+BVyU5rZsWWQPsmHCd4zLKeEny\nfOCnGIx9qRplrDuB18Dg/5rBzn8p7hRHed4+v7uNJG8G/rGqvj35UifiQeAN8Ny3LjxTVQfmW8Ep\nII59glqSX+tu/3Pgl4BfT3II+A6DV4pL0ijjraqdSbYAnwOeBd5XVUsyAEZ8fAGuAz5eVd+dUqkn\nbcSx/gHwgSRPMHgR+DtL8cXMiGNdDdzbfcXM54F1Uyv4JCW5H7gSeEGSrwB3MJiqPfyc3Zzk6iS7\ngf8B3rjgNruPDEmSGuMUkCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlR/wdDWEOh\nLJmW/QAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7fc90c8344d0>"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- - -\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "For real!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "Now that we have an idea of what can happen in a searchlight analysis,\n",
      "let's do another one, but this time on a more familiar ROI -- the full brain."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- - -\n",
      "**Exercise**\n",
      "\n",
      "\n",
      "\n",
      "Load the dataset with `get_haxby2001_data_alternative(roi='brain')`\n",
      "this will apply any required preprocessing for you. Now run a searchlight\n",
      "analysis for radii 0, 1 and 3. For each resulting error map look at the\n",
      "distribution of values, project them back into the fMRI volume and\n",
      "compare them. How does the distribution change with radius and how does\n",
      "it compare to results of the previous exercise? What would be a good\n",
      "choice for the threshold in this case?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# you can use this cell for this exercise\n",
      "ds = get_haxby2001_data_alternative(roi='brain')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- - -\n",
      "\n",
      "\n",
      "You have now performed a number of searchlight analyses, investigated the\n",
      "results and probably tried to interpret them. What conclusions did you draw\n",
      "from these analyses in terms of the neuroscientific aspects? What have you\n",
      "learned about object representation in the brain? In this case we have run\n",
      "8-way classification analyses and have looked at the average error rate across\n",
      "all conditions in thousands of sphere-shaped ROIs in the brain. In some spheres the\n",
      "classifier could perform well, i.e. it could predict all\n",
      "samples equally well. However, this only applies to a handful of over 30k\n",
      "spheres we have tested, and does not reveal whether the classifier was capable of\n",
      "classifying *all* of the conditions or just some.  For the vast majority\n",
      "we observe errors somewhere\n",
      "between the theoretical chance level and zero and we don't know what caused\n",
      "the error to decrease. We don't even know which samples get misclassified.\n",
      "\n",
      "From *chap_tutorial_classifiers* we know that there is a way out of this\n",
      "dilemma. We can look at the confusion matrix of a classifier to get a lot more\n",
      "information that is otherwise hidden. However, we cannot reasonably do this for\n",
      "thousands of searchlight spheres (Note that this is not completely true. See\n",
      "e.g. *Connolly et al., 2012* for some creative use-cases for\n",
      "searchlights).  It becomes obvious that a searchlight analysis is probably not\n",
      "the end of a data exploration but rather a crude take off, as it raises more\n",
      "questions than it answers.\n",
      "\n",
      "Moreover, a searchlight cannot detect signals that extend beyond a small\n",
      "local neighborhood. This property effectively limits the scope of analyses\n",
      "that can employ this strategy. A study looking a global brain circuitry\n",
      "will hardly restrict the analysis to patches of a few cubic millimeters of\n",
      "brain tissue. As we have seen before, searchlights also have another nasty\n",
      "aspect. Although they provide us with a multivariate localization measure,\n",
      "they also inherit the curse of univariate fMRI data analysis --\n",
      "[multiple comparisons](http://en.wikipedia.org/wiki/Multiple_comparisons). PyMVPA comes with an algorithm that can help to\n",
      "cope with the problem in the context of group analyses:\n",
      "[GroupClusterThreshold](http://pymvpa.org/generated/mvpa2.algorithms.group_clusterthr.GroupClusterThreshold.html#mvpa2-algorithms-group-clusterthr-groupclusterthreshold).\n",
      "\n",
      "Despite these limitations a searchlight analysis can be a valuable exploratory\n",
      "tool if used appropriately. The capabilities of PyMVPA's searchlight\n",
      "implementation go beyond what we looked at in this tutorial. It is not only\n",
      "possible to run *spatial* searchlights, but multiple spaces can be considered\n",
      "simultaneously. This is further illustrated in\n",
      "*chap_tutorial_eventrelated_searchlight*."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}